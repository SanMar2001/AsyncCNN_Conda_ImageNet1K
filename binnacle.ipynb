{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6996156",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paquetes iniciales para el funcionamiento de todo el cuaderno\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import asyncio\n",
    "from torch.optim import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dece3394",
   "metadata": {},
   "source": [
    "## Bitácora desarrollo de red neuronal con entrenamiento distribuído asíncrono\n",
    "En este cuaderno se encuentra el proceso de desarrollo y el paso desde nuestra versión semiasíncrona hasta llegar a la versión completamente asíncrona y las consideraciones y retos dentro del proceso\n",
    "\n",
    "### Planteamiento inicial\n",
    "Desde el punto de partida contábamos con una versión que utilizaba Docker para el despliegue con el dataset CIFAR10, realizando pruebas locales desplegando hasta 4 contenedores que realizaban el entrenamiento comunicando todo mediante sockets. Dentro de esta versión contábamos con un parameter server que de forma asíncrona esperaba solicitudes por parte de workers, para entregar a estos lotes de trabajo, sin embargo la optimización la realizaba de forma sincrónica ya que acumulaba los gradientes de todos los workers activos antes de optimizar, para así evitar gradientes obsoletos. Este fue nuestro punto de partida, ya que el propósito de esta segunda implementación era llegar a una versión completamente asincrónica y primero recurrimos a la literatura, basándonos así en los siguientes recursos como punto de partida:\n",
    "- Entrenamiento asíncrono con SGD (https://arxiv.org/abs/1511.05950)\n",
    "- Comportamiento del momentum en ASGD (https://arxiv.org/abs/1907.11612)\n",
    "\n",
    "### Primeros pasos\n",
    "Después de recurrir a la literatura vimos una pequeña ventaja desde el punto de partida. Nuestra versión previa ya calculaba los gradientes de forma asíncrona pero perdía el factor asíncrono en el momento en que acumulaba los gradientes antes de optimizar, así que las modificaciones para la versión asíncrona eran simples. Partimos de modificar primero nuestro modelo de parameter server, teniendo los siguientes atributos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9fe6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParameterServer:\n",
    "    def __init__(self, id, host, port, model, longitude,\n",
    "                 epochs=50, batch_size=128, lr=0.08, alpha=1.0):\n",
    "        self.id = id\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        self.model = model\n",
    "        self.longitude = longitude\n",
    "        self.indexes = np.random.permutation(longitude).tolist()\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_pointer = 0\n",
    "        self.base_lr = lr\n",
    "        self.alpha = alpha\n",
    "        self.optimizer = SGD(self.model.parameters(), lr, momentum=0.0)\n",
    "        self.epochs = epochs\n",
    "        self.current_epoch = 0\n",
    "        self.server = None\n",
    "        self.connections = {}\n",
    "        self.model_version = 0\n",
    "        self.time = None\n",
    "        self.lock = asyncio.Lock()\n",
    "        self.active_workers = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49d341b",
   "metadata": {},
   "source": [
    "Vemos una variable llamada model_version, que en este caso será la encargada de conservar la versión del modelo, ya que este valor será esencial para nuestro entrenamiento asíncrono. Primero es importante entender como funciona este entrenamiento asíncrono. Para entrenamiento asíncronos hablamos de **staleness** para referirnos al desfase que genera actualizar un modelo con gradientes obsoletos, por lo tanto la manera en que se controla dicho desfase, es manejando la versión del modelo y calculando el **staleness** como la versión global del modelo menos la versión local que traigan los gradientes a la hora de optimizar. En base a este valor obtenemos la siguiente fórmula para el control de dicho desfase:\n",
    "$$\n",
    "\\text{async\\_lr} = \\frac{\\text{base\\_lr}}{1 + \\alpha \\cdot \\text{staleness}}\n",
    "$$\n",
    "Básicamente lo que hacemos es tomar el valor de learning rate y entre mayor *staleness* tengamos, menor será el learning rate, disminuyendo así la manera en que los gradientes afectaran, controlando así el desfase. Ahora bien también tenemos un parámetro $\\alpha$, que definimos como la severidad que tendrá el **staleness** que para nuestro caso lo mantenemos en 1 como se puede ver desde los atributos del parameter server.\n",
    "\n",
    "Para tener mayor claridad veremos en código la manera en que se actualizan los gradientes dentro de nuestra implementación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e704751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import MSG_GRADIENTS, msg_type, message, self\n",
    "\n",
    "if msg_type == MSG_GRADIENTS:\n",
    "    worker_version = message.get(\"model_version\", 0)\n",
    "    grads = message.get(\"data\")\n",
    "    staleness = max(0, self.model_version - worker_version)\n",
    "    async_lr = self.base_lr / (1.0 + self.alpha * staleness)\n",
    "\n",
    "    async with self.lock:\n",
    "        self.optimizer.param_groups[0]['lr'] = async_lr\n",
    "        self.optimizer.zero_grad()\n",
    "        for param, grad in zip(self.model.parameters(), grads):\n",
    "            param.grad = grad.clone().to(param.device)\n",
    "        self.optimizer.step()\n",
    "        self.model_version += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71800f6",
   "metadata": {},
   "source": [
    "Básicamente tenemos un tipo de mensaje que el parameter server distingue al recibir gradientes por parte de un worker, donde obtiene la versión del modelo para calcular el **staleness** y mediante este desfase actualiza el learning rate para el optimizador que teníamos definido desde los atributos que vimos previamente. En base a esta actualización del learning rate carga los gradientes recibidos y realiza el step con el optimizador preparado, aumentando también la versión del modelo. Si nos fijamos a la hora de realizar estos procesos lo hace mediante un self.lock, esto para que no se generen condiciones de carrera con los workers a la hora de querer optimizar el modelo global, ya que este lock básicamente se encarga de bloquear dicho segmento para ser accesible solo por un worker al tiempo.\n",
    "\n",
    "Ahora teniendo aplicado el funcionamiento observado en el primer paper, pasamos al segundo paper, donde se habla del momentum. El momentum en nuestra primera implementación tenía valores altos, entre 0.8 y 0.9, sin embargo el paper establece que para modelos asíncronos los valores de momentum altos complican el entrenamiento debido al comportamiento asíncrono, lo cual pudimos comprobar al punto de dejar nuestro momentum en 0.0, ya que probando valores de hasta 0.1 afectaba en gran medida el accuracy final del modelo.\n",
    "\n",
    "### Despliegue de la implementación\n",
    "En nuestra primera implementación usamos Docker, sin embargo no fue tan sencillo, ya que para poder usar GPU's fue necesario primero instalar el toolkit de NVIDIA en el kernel de Docker, que para nuestro caso al ser en Windows, debimos instalar primero un WSL, en este WSL instalar el toolkit y usar Docker basado en dicho WSL. Para nuestra segunda implementación intentamos usar **Conda**, sin embargo el primer problema fue el funcionamiento de **Conda** en Windows, el cual tiene una gran cantidad de errores gracias a la incompatibilidad con ciertos DLL's. Probamos también mediante el uso del clásico **venv** para así usar entornos virtuales simples, que en un inicio eran completamente funcionales pero al usar **venv** en Windows no se puede establecer ningún control sobre el uso de RAM y demás, por lo tanto las pruebas terminaban siendo bastante complejas porque los entornos virtuales comenzaban a consumir recursos sin control. Después de todo esto optamos por continuar con Docker inicialmente, ya que parte de esta segunda implementación era el cambio de dataset, lo cual veremos a continuación.\n",
    "\n",
    "### Cambio de Dataset a ImageNet1k\n",
    "Uno de los mayores retos para nosotros fue el cambio de dataset, ya que en un inicio, cuando realizamos los primeros cambios implementando el manejo del **staleness** y el momentum, todo fue probado con el dataset de CIFAR10, obteniendo valores en accuracy por encima el 40%, lo cual en un inicio nos pareció un buen punto de partida. Sin embargo a la hora de cambiar el dataset corrimos con varias complicaciones; primero la magnitud del dataset era mucho mayor, teniendo presente que CIFAR10 tiene un peso menor a 3GB e ImageNet1k pesaba entre 60-70GB en su versión más liviana que se encuentra en Kaggle. Igualmente descargamos el dataset y modificamos nuestras capas convolucionales para poder adaptarlas a este problema. A continuación veremos nuestro modelo para CIFAR10 y el modelo que adaptamos para ImageNet1k:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebc6cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolutional_ImageNet(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((7, 7))\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512 * 7 * 7, 8192),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(8192, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(4096, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Linear(2048, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Convolutional_CIFAR10(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128*4*4, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.15),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a69d8f",
   "metadata": {},
   "source": [
    "De entrada basta con solo mirar las capas convolucionales para ver la forma en que aumentó la amplitud del modelo, ya que en ImageNet1k procesamos imágenes de 224x224, siendo CIFAR10 de imágenes de solo 32x32. Gracias a esto la capa de clasificación también aumentó bastante, generando todo esto bloqueos en la máquina sobre la cual realizamos las pruebas, teniendo que disminuir el tamaño de los batches bastante para poder al menos correr el código sin tener bloqueos. Después de tener varios inconvenientes con el manejo del dataset, podíamos empezar a ver que teníamos un problema relacionado con la capacidad de cómputo y no con el planteamiento del algoritmo, ya que en CIFAR10 veíamos una base correcta como punto de partida. Decidimos explorar más datasets que se encontraran en un punto medio entre CIFAR10 e ImageNet1k y encontramos Food101, el cual clasifica alimentos, pero también tiene imágenes de tamaños variados, que recomiendan usar a 224x224, teniendo un tamaño igual al anterior pero reducciones la capa de clasificación como lo veremos a continuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6561ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolutional_Food101(nn.Module):\n",
    "    def __init__(self, num_classes=101):\n",
    "        super().__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.AdaptiveAvgPool2d((7, 7))\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.1), \n",
    "            nn.Linear(4096, 1024), \n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(0.1), \n",
    "            nn.Linear(1024, 512), \n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(0.1), \n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120e5db9",
   "metadata": {},
   "source": [
    "En este modelo nuestra capa de clasificación es un poco más pequeña pero la capa convolucional mantiene la misma complejidad. Con estos arreglos intentamos correr pruebas, y para este caso si fue posible hacerlo pero con tiempos de 40-50 minutos por época, además teniendo presente el hecho de que todas las pruebas se corrieron usando GPU (NVIDIA RTX 3060 12GB), sin embargo el uso de la GPU al ser todo mediante pruebas locales no era el más óptimo debido a que el uso de la GPU se hace mediante una cola de ejecución.\n",
    "\n",
    "### Conclusiones\n",
    "A partir de todas las pruebas realizadas pudimos concluir distintas cosas, entre ellas las siguientes:\n",
    "1. El planteamiento para el control del **staleness** en modelos asíncronos fue adecuado, ya que en las primeras pruebas con CIFAR10 vimos una base adecuada reflejada en el accuracy probando distintos hiperparámetros.\n",
    "2. En las pruebas realizadas con CIFAR10 observamos que el learning rate con valores más altos obtiene mejores resultados y tiene bastante sentido, ya que si de entrada el learning rate tiene valores muy bajos no tendrá tantas diferencias el dividirlo sobre el **staleness**\n",
    "3. En entrenamiento asíncrono el se debe tener mucho cuidado con el dropout y con el momentum, ya que en modelos síncronos nos pueden ayudar mucho pero en modelos asíncronos pueden generar pérdida en la tendencia que necesitamos captar siendo esto primordial para estos casos.\n",
    "4. El uso de datasets más grandes si requiere el uso de cómputo distribuido real sea en la nube o de forma física para tener pruebas adecuadas, sin embargo la limitante mayormente por esto, por el acceso a capacidad de cómputo.\n",
    "\n",
    "\n",
    "- *Santiago Martínez Varón - 1004521370*\n",
    "- *Luis Miguel Salazar Londoño - 1192725457*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
